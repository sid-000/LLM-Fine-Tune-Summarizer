# ğŸ§  LLM Fine-Tuning for Text Summarization

This project fine-tunes a pretrained T5 model on the XSum dataset for abstractive text summarization using the Hugging Face Transformers library. It demonstrates training, evaluation, and generating summaries with custom inputs.

## ğŸ› ï¸ Tools & Libraries
- ğŸ¤— Hugging Face Transformers
- PyTorch
- Datasets (XSum)
- ROUGE Evaluation
- Jupyter Notebooks

## ğŸ“ Project Structure

summarizer/
â”œâ”€â”€ train.py # Fine-tuning script using Trainer API
â”œâ”€â”€ evaluate.py # Script to generate predictions & ROUGE scores
â”œâ”€â”€ config.json # Training configuration parameters

outputs/
â”œâ”€â”€ t5_finetuned_xsum
â”œâ”€â”€ rouge_scores.png # ROUGE evaluation chart or metrics image
â”œâ”€â”€ sample_preds.txt # Example model predictions on test data




## ğŸš€ Getting Started

1. Install dependencies:
```bash
pip install -r requirements.txt

2. Run training:
python summarizer/train.py

3. Evaluate model:
python summarizer/evaluate.py


ğŸ“ˆ Sample ROUGE Scores

ROUGE-1: 43.2

ROUGE-2: 20.1

ROUGE-L: 40.7


ğŸ“ Sample Input & Output

Input: "The government is planning to announce new tax reforms..."
Generated Summary: "The government is preparing new tax reforms."


ğŸ—“ï¸ Project Timeline

Originally Started: Nov 2023

Finalized: Jan 2024

Portfolio Update: July 2025


ECHO is on.
# Portfolio Update: March 2024 
