# ğŸ§  LLM Fine-Tuning for Text Summarization

This project fine-tunes a pretrained T5 model on the XSum dataset for abstractive text summarization using the Hugging Face Transformers library. It demonstrates training, evaluation, and generating summaries with custom inputs.

## ğŸ› ï¸ Tools & Libraries
- ğŸ¤— Hugging Face Transformers
- PyTorch
- Datasets (XSum)
- ROUGE Evaluation
- Jupyter Notebooks

## ğŸ“ Project Structure

<pre>
ğŸ“ Project Structure

summarizer/
â”œâ”€â”€ train.py            # Fine-tuning script using Hugging Face Trainer API
â”œâ”€â”€ evaluate.py         # Script to evaluate model & generate ROUGE scores
â”œâ”€â”€ config.json         # Training configuration

outputs/
â””â”€â”€ t5_finetuned_xsum/
    â”œâ”€â”€ rouge_scores.png     # ROUGE evaluation metrics image
    â”œâ”€â”€ sample_preds.txt     # Sample model predictions on test data
</pre>


## ğŸš€ Getting Started

Follow these steps to set up and run the summarization pipeline:

---

### ğŸ“¦ **Step 1: Install Dependencies**

```bash
pip install -r requirements.txt
```

---

### ğŸ‹ï¸â€â™‚ï¸ **Step 2: Run Training**

```bash
python summarizer/train.py
```

---

### ğŸ“Š **Step 3: Evaluate the Model**

```bash
python summarizer/evaluate.py
```

---

### ğŸ§ª **Sample ROUGE Scores**

- **ROUGE-1**: 43.2  
- **ROUGE-2**: 20.1  
- **ROUGE-L**: 40.7  

---

### ğŸ” **Sample Input & Output**

**Input:**  
`"The government is planning to announce new tax reforms..."`  

**Generated Summary:**  
`"The government is preparing new tax reforms."`

---

### ğŸ—“ï¸ **Project Timeline**

- **Originally Started:** Jan 2024  
- **Finalized:** Mar 2024  
- **Portfolio Update:** July 2025
ğŸ—“ï¸ **Project Timeline**
